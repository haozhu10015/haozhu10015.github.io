<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="/jemdoc.css" type="text/css" />
<title>Inverse reinforcement learning via convex optimization</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<link rel="icon" href="/img/zhicon.png" type="image/x-icon" />
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Hao Zhu</div>
<div class="menu-item"><a href="../index.html">Home</a></div>
<div class="menu-item"><a href="../biography.html">Biography</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="../papers.html">Papers</a></div>
<div class="menu-item"><a href="../software.html">Software</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="../teaching/pgm.html">PGM</a></div>
<div class="menu-item"><a href="../teaching/realalys.html">Real&nbsp;Analysis</a></div>
</td>
<td id="layout-content">
<h2>Inverse reinforcement learning via convex optimization</h2>
<p><i>H. Zhu, Y. Zhang, and J. Boedecker</i>
</p>
<p><i>Manuscript</i>, January 2025.
</p>
<ul>
<li><p><a href="../pdf/cvx_irl/cvx_irl_paper.pdf" target=&ldquo;blank&rdquo;>Current version</a>
</p>
</li>
<li><p><a href="https://doi.org/10.48550/arXiv.2501.15957" target=&ldquo;blank&rdquo;>arXiv entry</a>
</p>
</li>
<li><p><a href="../pdf/cvx_irl/cvx_irl_talk.pdf" target=&ldquo;blank&rdquo;>Slides</a>
</p>
</li>
<li><p><a href="https://github.com/nrgrp/cvx_irl" target=&ldquo;blank&rdquo;>Code</a>
</p>
</li>
</ul>
<p>We consider the inverse reinforcement learning (IRL) problem, where an unknown reward function of some Markov decision process is estimated based on observed expert demonstrations.
In most existing approaches, IRL is formulated and solved as a nonconvex optimization problem, posing challenges in scenarios where robustness and reproducibility are critical.
We discuss a convex formulation of the IRL problem (CIRL) initially proposed by Ng and Russel, and reformulate the problem such that the domain-specific language CVXPY can be applied directly to specify and solve the convex problem.
We also extend the CIRL problem to scenarios where the expert policy is not given analytically but by trajectory as state-action pairs, which can be strongly inconsistent with optimality, by augmenting some of the constraints.
Theoretical analysis and practical implementation for hyperparameter auto-selection are introduced.
This note helps the users to easily apply CIRL for their problems, without background knowledge on convex optimization.
</p>
<div id="footer">
<div id="footer-text">
Page generated 2025-10-30 22:46:19 CET, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
