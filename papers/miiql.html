<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="/jemdoc.css" type="text/css" />
<title>Multi-intention inverse Q-learning for interpretable behavior representation</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<link rel="icon" href="/img/zhicon.png" type="image/x-icon" />
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Hao Zhu</div>
<div class="menu-item"><a href="../index.html">Home</a></div>
<div class="menu-item"><a href="../biography.html">Biography</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="../papers.html">Papers</a></div>
<div class="menu-item"><a href="../software.html">Software</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="../teaching/pgm.html">PGM</a></div>
<div class="menu-item"><a href="../teaching/realalys.html">Real&nbsp;Analysis</a></div>
</td>
<td id="layout-content">
<h2>Multi-intention inverse Q-learning for interpretable behavior representation</h2>
<p><i>H. Zhu, B. De La Crompe, G. Kalweit, A. Schneider, M. Kalweit, I. Diester, and J. Boedecker</i>
</p>
<p><i>Transactions on Machine Learning Research</i>, September 2024.
</p>
<ul>
<li><p><a href="../pdf/miiql/miiql_paper.pdf" target=&ldquo;blank&rdquo;>Final version</a>
</p>
</li>
<li><p><a href="https://openreview.net/forum?id=hrKHkmLUFk" target=&ldquo;blank&rdquo;>OpenReview entry</a>
</p>
</li>
<li><p><a href="https://doi.org/10.48550/arXiv.2311.13870" target=&ldquo;blank&rdquo;>arXiv entry</a>
</p>
</li>
<li><p><a href="../pdf/miiql/miiql_slides.pdf" target=&ldquo;blank&rdquo;>Slides</a>
</p>
</li>
<li><p><a href="https://github.com/nrgrp/hiql" target=&ldquo;blank&rdquo;>Code</a>
</p>
</li>
<li><p><a href="https://www.youtube.com/watch?v=0u-fboAO6-I" target=&ldquo;blank&rdquo;>Video</a>
</p>
</li>
</ul>
<p>In advancing the understanding of natural decision-making processes, inverse reinforcement learning (IRL) methods have proven instrumental in reconstructing animal's intentions underlying complex behaviors.
Given the recent development of a continuous-time multi-intention IRL framework, there has been persistent inquiry into inferring discrete time-varying rewards with IRL.
To address this challenge, we introduce the class of hierarchical inverse Q-learning (HIQL) algorithms.
Through an unsupervised learning process, HIQL divides expert trajectories into multiple intention segments, and solves the IRL problem independently for each.
Applying HIQL to simulated experiments and several real animal behavior datasets, our approach outperforms current benchmarks in behavior prediction and produces interpretable reward functions.
Our results suggest that the intention transition dynamics underlying complex decision-making behavior is better modeled by a step function instead of a smoothly varying function.
This advancement holds promise for neuroscience and cognitive science, contributing to a deeper understanding of decision-making and uncovering underlying brain mechanisms.
</p>
<div id="footer">
<div id="footer-text">
Page generated 2025-10-30 22:46:19 CET, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
